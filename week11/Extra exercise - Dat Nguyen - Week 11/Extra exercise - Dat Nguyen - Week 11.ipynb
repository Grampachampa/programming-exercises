{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47257504",
   "metadata": {},
   "source": [
    "### An improved classifier: Naive Bayes \n",
    "\n",
    "Welcome to the new company, the first task was for you to make a spam classifier to help with the amount of advertisement emails that the company had received lately. Remember that we are actually paying you for this (unlike your previous employer that fired you), so put extra effort into it.\n",
    "\n",
    "Before you start, take some time to look at this explanation of what a Naive Bayes classifier is:\n",
    "https://www.geeksforgeeks.org/naive-bayes-classifiers/\n",
    "\n",
    "To explain it in a few lines, Naive Bayes classifier classify a certain \"thing\", given some prior probability:\n",
    "- Thing A (that is made up of A1, A2, A3) is classified as B instead of C iff P(A1|B) \\* P(A2|B) \\* P(A3|B) > P(A1|C) \\* P(A2|C) \\* P(A3|C).\n",
    "- You have noticed that the classifier would make strong assumptions, but we would not discuss that now, and leave that to the other course \\*cough\\* totally not IS.\n",
    "\n",
    "Your classifier should take a txt file as an input, and return a boolean, True if the content of the file is spam, and False if the content of the file is ham.\n",
    "\n",
    "The txt files would be in the format of:\n",
    "\n",
    "\"Train-Spam \\n\n",
    " Content\" if it is to be used for training the classifier.\n",
    " \n",
    " and \n",
    " \n",
    " \"Classify \\n\n",
    " Content\" if it needs classification.\n",
    " \n",
    " The classifier should have a memory that is in the form of a nested dictionary: \n",
    " {\"Spam\": {\"word\": frequency}, \"Ham\": {\"word\": frequency}}\n",
    " \n",
    "It is relatively easy to implement the training part of the classifier: simply enumerate the words in the file, if the file is classified as Spam, then put the words and their occurences into the spam sub-dictionary; else put them in the Ham sub-dictionary.\n",
    " \n",
    "Classification should work as follow:\n",
    "- For each word in the file:\n",
    "-   Calculate their probability of being spam P(word|spam) and their probability of being ham P(word|ham).\n",
    "- Take the product of the probabilities and compare them: ΠP(word|spam) and ΠP(word|ham)\n",
    "\n",
    "#### It might be the case that sometimes the probability of a word you get is 0 (the word doesn't exisits in the memory), taking the product with the value would collapse the classifier (making the whole term 0). A simple way to solve this issue is just to ignore the word (making both P(word|ham) and P(word|spam) 1); although this solve the problem, it is equivalent to assigning the probability 1 to the word. A better (but lets ignore it for now) approach would be to use Laplace smoothing (https://towardsdatascience.com/introduction-to-na%C3%AFve-bayes-classifier-fa59e3e24aaf)\n",
    "\n",
    "If you are looking at the answer, it might be helpful to look at this description of a static method; it is simply a method that you defined in a class, but you can call it even for non-class instances: https://www.digitalocean.com/community/tutorials/python-static-method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a357fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class naiveBayes():\n",
    "    def __init__(self, memory = None):\n",
    "        if memory is None:\n",
    "            self.memory = {\"Spam\": {}, \"Ham\": {}}\n",
    "        else:\n",
    "            self.memory = memory\n",
    "        \n",
    "        self.indicator = {True: \"Spam\", False: \"Ham\"}\n",
    "            \n",
    "    def train(self, flag, data):\n",
    "        for word in data:\n",
    "            occurance = self.memory[flag].get(word, 0)\n",
    "            occurance += 1\n",
    "            self.memory[flag][word] = occurance\n",
    "        \n",
    "        print(f\"trained as {flag}\")\n",
    "    \n",
    "    def classify(self, data):\n",
    "        prob_spam = 1\n",
    "        prob_ham = 1\n",
    "        for word in data:\n",
    "            occur_ham = self.memory[\"Ham\"].get(word, None)\n",
    "            occur_spam = self.memory[\"Spam\"].get(word, None)\n",
    "            \n",
    "            if (occur_ham is None) or (occur_spam is None):\n",
    "                occur_ham, occur_spam = 1, 1\n",
    "                freq = 1\n",
    "            else:\n",
    "                freq = occur_ham + occur_spam\n",
    "                \n",
    "            prob_spam *= (occur_spam/freq)\n",
    "            prob_ham *= (occur_ham/freq)\n",
    "        \n",
    "        if prob_spam > prob_ham:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def run(self, raw_data):\n",
    "        train_or_classify, flag, data = naiveBayes.parse_and_clean(raw_data)\n",
    "        if train_or_classify == \"Train\":\n",
    "            self.train(flag, data)\n",
    "            return None\n",
    "        else:\n",
    "            result = self.classify(data)\n",
    "            print(f\"Classified as {self.indicator[result]}\")\n",
    "            return result\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def parse_and_clean(raw):\n",
    "        head, body = raw.split(\"\\n\")\n",
    "        body = body.split(\" \")\n",
    "        for word, index in enumerate(body):\n",
    "            if type(word) == int:\n",
    "                continue\n",
    "            body[index] = \"\".join([letter for letter in word if letter.isalnum()])\n",
    "            body[index] = body[index].lower()\n",
    "        head = head.split(\"-\")\n",
    "        if len(head) == 1:\n",
    "            to_do = head[0]\n",
    "            return to_do, _, body\n",
    "        else:\n",
    "            to_do, flag = head\n",
    "            return to_do, flag, body\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df1913c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained as Ham\n",
      "trained as Spam\n",
      "trained as Ham\n",
      "trained as Spam\n",
      "Classified as Spam\n",
      "Classified as Spam\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "classifier = naiveBayes()\n",
    "\n",
    "files = glob.glob('./*.txt')\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read()\n",
    "        classifier.run(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
